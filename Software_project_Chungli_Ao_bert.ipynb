{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hR7MjnaCKvUo",
    "outputId": "9b126d86-4a1e-4c49-b455-77342f15dd9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.43.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenao\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.43.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\lenao\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenao\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not find a version that satisfies the requirement torch==1.11.00.676617 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\n",
      "ERROR: No matching distribution found for torch==1.11.00.676617\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install transformers accelerate\n",
    "!pip install torch==1.11.00.676617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b10T2NjDKB_D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R9RRbYypKZ0Q"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_chungli_ao = pd.read_csv(\"data/Chungli_Ao_train_set.csv\")\n",
    "df_english = pd.read_csv(\"data/English_dataset.csv\")\n",
    "df_german = pd.read_csv(\"data/german_sentiment.csv\", sep=\";\")\n",
    "df_mizo = pd.read_csv(\"data/Mizo_Training_Software_project.csv\")\n",
    "df_russian = pd.read_csv(\"data/Russian_fixed.csv\")\n",
    "df_telugu = pd.read_csv(\"data/Telugu_soft_ware_project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "adgkjcMaKfXw",
    "outputId": "2c10304a-8640-45da-ac91-0440f85dedd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe working on Chungli AO\\n\\n1 step: Data Collection.\\n2 step: Code  (Traditional ML vs pretrained models (XLMR,MBERT)\\n3 step: Multilingual data augmentation.English,German,Telugu, Russian and  Mizo is used in the training set.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We working on Chungli AO\n",
    "\n",
    "1 step: Data Collection.\n",
    "2 step: Code  (Traditional ML vs pretrained models (XLMR,MBERT)\n",
    "3 step: Multilingual data augmentation.English,German,Telugu, Russian and  Mizo is used in the training set.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7f8yUHhnKkNF"
   },
   "outputs": [],
   "source": [
    "# Concatenate the datasets\n",
    "df_chungli_english = pd.concat([df_chungli_ao, df_english], ignore_index=True)\n",
    "df_chungli_german = pd.concat([df_chungli_ao, df_german], ignore_index=True)\n",
    "df_chungli_mizo = pd.concat([df_chungli_ao, df_mizo], ignore_index=True)\n",
    "df_chungli_russian = pd.concat([df_chungli_ao, df_russian], ignore_index=True)\n",
    "df_chungli_russian = df_chungli_russian[['Text', 'Sentiment']]\n",
    "df_chungli_telugu = pd.concat([df_chungli_ao, df_telugu], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNi8Py8byFpZ"
   },
   "outputs": [],
   "source": [
    "# change this variable to change the data you're working with\n",
    "df = df_chungli_mizo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7pxE8ShvKoQu"
   },
   "outputs": [],
   "source": [
    "# Converting the labels to numbers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "df['Sentiment'] = le.fit_transform(df.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Cc8Kdm0KpBO"
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataframe\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9diQvgsjMtZb",
    "outputId": "ff01e3be-602c-4c74-a77c-d8433a4fa1cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>Pu TBC-A sawi tha rem rem hle mai, ZPM go go</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>Ava sawidik chiang em,kan sawrkar thartur chu ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>laptop nung agüja aliba 47.4Whr mapang iba fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>He mess aiin student film production value tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>lir. Tarutsü nung iba sentong takok nguyonga a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>Political hruaitute hian strategic misinformat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>\"Ajisüaka, osang tajung kaji meshiranger, anun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>He hlaa harmonies te hian thlamuanna min pe a....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>achiajem mazüoktetba yimya tamajung melenshits...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>team kati angati medicalAR-i Mangakhi nung med...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10319 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "5051       Pu TBC-A sawi tha rem rem hle mai, ZPM go go          1\n",
       "4977  Ava sawidik chiang em,kan sawrkar thartur chu ...          1\n",
       "74    laptop nung agüja aliba 47.4Whr mapang iba fil...          1\n",
       "4573  He mess aiin student film production value tha...          0\n",
       "3217  lir. Tarutsü nung iba sentong takok nguyonga a...          0\n",
       "...                                                 ...        ...\n",
       "8244  Political hruaitute hian strategic misinformat...          0\n",
       "2138  \"Ajisüaka, osang tajung kaji meshiranger, anun...          1\n",
       "7098  He hlaa harmonies te hian thlamuanna min pe a....          1\n",
       "2902  achiajem mazüoktetba yimya tamajung melenshits...          0\n",
       "454   team kati angati medicalAR-i Mangakhi nung med...          1\n",
       "\n",
       "[10319 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNY0SYd-KrG1",
    "outputId": "f96e36a0-b92e-4988-897d-b21c23048e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True labels: 5158\n",
      "Number of Fake labels: 5161\n"
     ]
    }
   ],
   "source": [
    "# Count the number of true and fake labels\n",
    "True_labels = (df['Sentiment'] == 1).sum()\n",
    "fake_labels = (df['Sentiment'] == 0).sum()\n",
    "print(f\"Number of True labels: {True_labels}\")\n",
    "print(f\"Number of Fake labels: {fake_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L336FMenKtNP"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels_str, val_labels_str = train_test_split(list(df['Text']), list(df['Sentiment']), test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NXfW8f8Vn9qa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Blue7Bird/chungli_ao_bert_model\", model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Blue7Bird/chungli_ao_bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5jCuquFRK6-u"
   },
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NdyLF1AVszBS"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9Q6i7NogK87X"
   },
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class spanish_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s_zLICmkK_M-"
   },
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "train_labels = le.fit_transform(train_labels_str)\n",
    "val_labels = le.transform(val_labels_str)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = spanish_Dataset(train_encodings, train_labels)\n",
    "val_dataset = spanish_Dataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "w0atdec7LBWt"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use the representation of [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "w10EpAq-LELV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_CuQQLSYoNcM"
   },
   "outputs": [],
   "source": [
    "model_name = \"Blue7Bird/chungli_ao_bert_model\"\n",
    "num_labels = 2  # Adjust this based on your specific task\n",
    "custom_model = CustomClassifier(model_name, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "L1tXXUogLIfP"
   },
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'Accuracy': acc, 'F1': f1, 'Precision': precision, 'Recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d_Xo5Y1Z3RDd"
   },
   "outputs": [],
   "source": [
    "#train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ti4vj6YPLK4e",
    "outputId": "b15164d0-ed42-409d-96a4-343632af914e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lenao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|▏         | 10/551 [11:10<10:25:46, 69.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6986, 'grad_norm': 9.783676147460938, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "  2%|▏         | 10/551 [1:26:20<10:25:46, 69.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7084096074104309, 'eval_Accuracy': 0.500484496124031, 'eval_F1': 0.500484496124031, 'eval_Precision': 0.500484496124031, 'eval_Recall': 0.500484496124031, 'eval_runtime': 4510.0013, 'eval_samples_per_second': 0.458, 'eval_steps_per_second': 0.031, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 20/551 [1:37:27<17:48:16, 120.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7115, 'grad_norm': 10.184102058410645, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define training arguments\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=15,  # batch size per device during training\n",
    "    per_device_eval_batch_size=15,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    eval_steps=10\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=custom_model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # compute metrics function\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em0aduDOLNXS"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/Test_data_Chungli_ao.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwluiILdLPbG"
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fgScOX-LRPi"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OggcEg7qLTOe"
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8BQAUFQLVF7"
   },
   "outputs": [],
   "source": [
    "test_texts = list(test_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9gz9NxpLXUp"
   },
   "outputs": [],
   "source": [
    "test_df['Sentiment'] = le.fit_transform(test_df.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXI_xmRbLZIZ"
   },
   "outputs": [],
   "source": [
    "test_labels = test_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHAdgJ-_Lbhj"
   },
   "outputs": [],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stppnrvPLdhd"
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "test_dataset = spanish_Dataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5MJ-xzZLfyA"
   },
   "outputs": [],
   "source": [
    "test_metrics = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jfYGmm3Lick"
   },
   "outputs": [],
   "source": [
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE816mROZ4wv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
