{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hR7MjnaCKvUo",
    "outputId": "9b126d86-4a1e-4c49-b455-77342f15dd9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.43.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenao\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.43.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\lenao\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenao\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenao\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not find a version that satisfies the requirement torch==1.11.00.676617 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\n",
      "ERROR: No matching distribution found for torch==1.11.00.676617\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install transformers accelerate\n",
    "!pip install torch==1.11.00.676617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b10T2NjDKB_D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R9RRbYypKZ0Q"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_chungli_ao = pd.read_csv(\"data/Chungli_Ao_train_set.csv\")\n",
    "df_english = pd.read_csv(\"data/English_dataset.csv\")\n",
    "df_german = pd.read_csv(\"data/german_sentiment.csv\", sep=\";\")\n",
    "df_mizo = pd.read_csv(\"data/Mizo_Training_Software_project.csv\")\n",
    "df_russian = pd.read_csv(\"data/Russian_fixed.csv\")\n",
    "df_telugu = pd.read_csv(\"data/Telugu_soft_ware_project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "adgkjcMaKfXw",
    "outputId": "2c10304a-8640-45da-ac91-0440f85dedd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe working on Chungli AO\\n\\n1 step: Data Collection.\\n2 step: Code  (Traditional ML vs pretrained models (XLMR,MBERT)\\n3 step: Multilingual data augmentation.English,German,Telugu, Russian and  Mizo is used in the training set.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We working on Chungli AO\n",
    "\n",
    "1 step: Data Collection.\n",
    "2 step: Code  (Traditional ML vs pretrained models (XLMR,MBERT)\n",
    "3 step: Multilingual data augmentation.English,German,Telugu, Russian and  Mizo is used in the training set.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7f8yUHhnKkNF"
   },
   "outputs": [],
   "source": [
    "# Concatenate the datasets\n",
    "df_chungli_english = pd.concat([df_chungli_ao, df_english], ignore_index=True)\n",
    "df_chungli_german = pd.concat([df_chungli_ao, df_german], ignore_index=True)\n",
    "df_chungli_mizo = pd.concat([df_chungli_ao, df_mizo], ignore_index=True)\n",
    "df_chungli_russian = pd.concat([df_chungli_ao, df_russian], ignore_index=True)\n",
    "df_chungli_russian = df_chungli_russian[['Text', 'Sentiment']]\n",
    "df_chungli_telugu = pd.concat([df_chungli_ao, df_telugu], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNi8Py8byFpZ"
   },
   "outputs": [],
   "source": [
    "# change this variable to change the data you're working with\n",
    "df = df_chungli_mizo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7pxE8ShvKoQu"
   },
   "outputs": [],
   "source": [
    "# Converting the labels to numbers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "df['Sentiment'] = le.fit_transform(df.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Cc8Kdm0KpBO"
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataframe\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9diQvgsjMtZb",
    "outputId": "ff01e3be-602c-4c74-a77c-d8433a4fa1cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>Pu TBC-A sawi tha rem rem hle mai, ZPM go go</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>Ava sawidik chiang em,kan sawrkar thartur chu ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>laptop nung ag√ºja aliba 47.4Whr mapang iba fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>He mess aiin student film production value tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>lir. Taruts√º nung iba sentong takok nguyonga a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>Political hruaitute hian strategic misinformat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>\"Ajis√ºaka, osang tajung kaji meshiranger, anun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>He hlaa harmonies te hian thlamuanna min pe a....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>achiajem maz√ºoktetba yimya tamajung melenshits...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>team kati angati medicalAR-i Mangakhi nung med...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10319 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "5051       Pu TBC-A sawi tha rem rem hle mai, ZPM go go          1\n",
       "4977  Ava sawidik chiang em,kan sawrkar thartur chu ...          1\n",
       "74    laptop nung ag√ºja aliba 47.4Whr mapang iba fil...          1\n",
       "4573  He mess aiin student film production value tha...          0\n",
       "3217  lir. Taruts√º nung iba sentong takok nguyonga a...          0\n",
       "...                                                 ...        ...\n",
       "8244  Political hruaitute hian strategic misinformat...          0\n",
       "2138  \"Ajis√ºaka, osang tajung kaji meshiranger, anun...          1\n",
       "7098  He hlaa harmonies te hian thlamuanna min pe a....          1\n",
       "2902  achiajem maz√ºoktetba yimya tamajung melenshits...          0\n",
       "454   team kati angati medicalAR-i Mangakhi nung med...          1\n",
       "\n",
       "[10319 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNY0SYd-KrG1",
    "outputId": "f96e36a0-b92e-4988-897d-b21c23048e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True labels: 5158\n",
      "Number of Fake labels: 5161\n"
     ]
    }
   ],
   "source": [
    "# Count the number of true and fake labels\n",
    "True_labels = (df['Sentiment'] == 1).sum()\n",
    "fake_labels = (df['Sentiment'] == 0).sum()\n",
    "print(f\"Number of True labels: {True_labels}\")\n",
    "print(f\"Number of Fake labels: {fake_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L336FMenKtNP"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels_str, val_labels_str = train_test_split(list(df['Text']), list(df['Sentiment']), test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NXfW8f8Vn9qa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Blue7Bird/chungli_ao_bert_model\", model_max_length=512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Blue7Bird/chungli_ao_bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5jCuquFRK6-u"
   },
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NdyLF1AVszBS"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9Q6i7NogK87X"
   },
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class spanish_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s_zLICmkK_M-"
   },
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "train_labels = le.fit_transform(train_labels_str)\n",
    "val_labels = le.transform(val_labels_str)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = spanish_Dataset(train_encodings, train_labels)\n",
    "val_dataset = spanish_Dataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "w0atdec7LBWt"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use the representation of [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "w10EpAq-LELV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_CuQQLSYoNcM"
   },
   "outputs": [],
   "source": [
    "model_name = \"Blue7Bird/chungli_ao_bert_model\"\n",
    "num_labels = 2  # Adjust this based on your specific task\n",
    "custom_model = CustomClassifier(model_name, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "L1tXXUogLIfP"
   },
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'Accuracy': acc, 'F1': f1, 'Precision': precision, 'Recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d_Xo5Y1Z3RDd"
   },
   "outputs": [],
   "source": [
    "#train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ti4vj6YPLK4e",
    "outputId": "b15164d0-ed42-409d-96a4-343632af914e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lenao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|‚ñè         | 10/551 [11:10<10:25:46, 69.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6986, 'grad_norm': 9.783676147460938, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "  2%|‚ñè         | 10/551 [1:26:20<10:25:46, 69.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7084096074104309, 'eval_Accuracy': 0.500484496124031, 'eval_F1': 0.500484496124031, 'eval_Precision': 0.500484496124031, 'eval_Recall': 0.500484496124031, 'eval_runtime': 4510.0013, 'eval_samples_per_second': 0.458, 'eval_steps_per_second': 0.031, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñé         | 20/551 [1:37:27<17:48:16, 120.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7115, 'grad_norm': 10.184102058410645, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define training arguments\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=15,  # batch size per device during training\n",
    "    per_device_eval_batch_size=15,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    eval_steps=10\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=custom_model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # compute metrics function\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em0aduDOLNXS"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/Test_data_Chungli_ao.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwluiILdLPbG"
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fgScOX-LRPi"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OggcEg7qLTOe"
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8BQAUFQLVF7"
   },
   "outputs": [],
   "source": [
    "test_texts = list(test_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9gz9NxpLXUp"
   },
   "outputs": [],
   "source": [
    "test_df['Sentiment'] = le.fit_transform(test_df.Sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXI_xmRbLZIZ"
   },
   "outputs": [],
   "source": [
    "test_labels = test_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHAdgJ-_Lbhj"
   },
   "outputs": [],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stppnrvPLdhd"
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "test_dataset = spanish_Dataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5MJ-xzZLfyA"
   },
   "outputs": [],
   "source": [
    "test_metrics = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jfYGmm3Lick"
   },
   "outputs": [],
   "source": [
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE816mROZ4wv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
